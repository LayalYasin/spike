rule bwa:
    # Do the alignment, read paring, introduction of the read groups, sam-import and sorting on a pipe. Paired-end flavour
    input:
        forward="{prefix}%s%s/{run}/Paired/{sample}_R1.fastq.gz" % (config['dirs']['intermediate'], config['stepnames']['trim']),
        reverse="{prefix}%s%s/{run}/Paired/{sample}_R2.fastq.gz" % (config['dirs']['intermediate'], config['stepnames']['trim']),
        reference="{prefix}%shGRC37.fa" % (config['dirs']['references'])
    output:
        "{prefix}%s%s/{run,[^/]+XX}/{sample}.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map'])
    benchmark:
        "{prefix}%s{run}/{sample}.srt.bam.benchmark" % config['dirs']['benchmarks']
    log:
        "{prefix}%s{run}/{sample}.srt.bam.log" % config['dirs']['logs']
    conda:
        "envs/spike_map.yaml"
    threads:
        4
    params:
        run=config['run'],
        flowcell=config['run'].split('_')[-1],
        date='%04i/%02i/%02i' % (
            int(config['run'].split('_')[0][:2])+2000,
            int(config['run'].split('_')[0][3:4]),
            int(config['run'].split('_')[0][5:6]))
    shell:
        'bwa mem -t {threads} -v 2 -M'
        ' -R "@RG\\tID:{params.run}\\tCN:Department_of_Pediatric_Oncology_Dusseldorf\\tPU:{params.flowcell}\\tDT:{params.date}\\tPL:ILLUMINA\\tLB:SureSelectXTV5plusUTRautomated\\tSM:readgroups.info"'
        ' {input.reference}'
        ' {input.forward} {input.reverse} 2> {log}'
        ' | samtools view -bSu -F 0x04 - 2>> {log}'
        ' | samtools sort -m 1G -@ {threads} -o {output} 2>> {log}'

        #"@RG\tID:180614_SN737_0438_BCC7MCACXX\tCN:Department_of_Pediatric_Oncology_Dusseldorf\tPU:BCC7MCACXX\tDT:2018/06/14\tPL:ILLUMINA\tLB:SureSelectXTV5plusUTRautomated\tSM:readgroups.info"
        #bwa mem -t $(NT_MINUS_ONE) -v 1 -M $(shell getReadGroupStringFromFilename_2015.pl $*) $(REF) $*_R1.fastq.gz $*_R2.fastq.gz |
        #$(SAMTOOLS_PATH_2k15)/samtools view -bSu -F 0x04 - |
        #$(SAMTOOLS_PATH_2k15)/samtools sort -m 1G -@ 1 - $@.tmp     $(STDERR_TO_LOG) \
	    #&& mv $@.tmp.bam $@

# rule remove_pcr_duplicates:
#     # Remove PCR duplicates from the reads
#     input:
#         "{prefix}%s%s/{run}/{sample}.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['map'])
#     output:
#         bam="{prefix}%s%s/{run,[^/]+XX}/{sample}.nodup.srt.bam" % (config['dirs']['intermediate'], config['stepnames']['nodup']),
#         bai="{prefix}%s%s/{run,[^/]+XX}/{sample}.srt.bai" % (config['dirs']['intermediate'], config['stepnames']['nodup'])
#     shell:
#         "echo '{input} {output}'"
# %.nodup.srt.bam: %.srt.bam
# 	@echo "$(DATE) ######### Alignment: Removing PCR duplicates in '$<'"
# 	picardtools_2015.sh picard.jar MarkDuplicates INPUT=$< METRICS_FILE=$@.metrics REMOVE_DUPLICATES=true ASSUME_SORTED=true  VALIDATION_STRINGENCY=LENIENT CREATE_INDEX=true OUTPUT=$@.tmp  $(BOTH_TO_LOG) \
# 	&& mv $@.tmp $@
# 	-mv $@.tmp.bai $@.bai
# #  The $@.metrics file contains how many reads were processed / duplicate
